{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddedDllDirectory('C:\\\\OpenCV\\\\opencv-4.10.0\\\\build\\\\bin\\\\Release')>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.add_dll_directory(r\"C:\\OpenCV\\opencv-4.10.0\\build\\install\\x64\\vc16\\bin\")\n",
    "os.add_dll_directory(r\"C:\\OpenCV\\opencv-4.10.0\\build\\bin\\Release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import argparse\n",
    "import networkx as nx\n",
    "\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchWithRatioTest(\n",
    "    matcher: cv.DescriptorMatcher, desc1, desc2, nn_match_ratio_th=0.8\n",
    "):\n",
    "    nn_matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "    ratioMatched = []\n",
    "    for m, n in nn_matches:\n",
    "        if m.distance < n.distance * nn_match_ratio_th:\n",
    "            ratioMatched.append(m)\n",
    "\n",
    "    return ratioMatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clase Imagen\n",
    "\n",
    "Almacena el nombre de la imagen, keypoints, descriptores y un diccionario con los emparejamientos que tiene.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Image:\n",
    "    def __init__(self, img_name, image, keypoints, descriptors):\n",
    "        self.img_name = img_name\n",
    "        self.image = image\n",
    "        self.keypoints = keypoints\n",
    "        self.descriptors = descriptors\n",
    "        self.matches = {}\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.img_name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Imagen: {self.img_name} \\n # Keypoints: {len(self.keypoints)} \\n # Descriptores: {self.descriptors.shape} \\n Matches: {list(self.matches.keys())} \\n\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Clase ImageFeature\n",
    "Almacena la referencia a una imagen (nombre del archivo) junto con un índice del keypoint específico a esa imagen\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ImageFeature:\n",
    "    def __init__(self, img_name, kpt_idx):\n",
    "        self.img_name = img_name\n",
    "        self.kpt_idx = kpt_idx\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.img_name + str(self.kpt_idx))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.img_name}, keypoint: {self.kpt_idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIR_MATCH_SURVIVAL_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints: 1837, descriptors: (1837, 61)\n",
      "keypoints: 2171, descriptors: (2171, 61)\n",
      "keypoints: 2166, descriptors: (2166, 61)\n",
      "keypoints: 1996, descriptors: (1996, 61)\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de imágenes\n",
    "images = {}\n",
    "\n",
    "# Carpeta de las imagenes\n",
    "images_folder = \"./images\"\n",
    "\n",
    "# Leer todos los archivos dentro de la carpeta\n",
    "images_files = listdir(images_folder)\n",
    "\n",
    "# Inicializar descriptor AKAZE\n",
    "akaze = cv.AKAZE_create()\n",
    "# Almacenamos todos los keypoints y descriptors que encontramos\n",
    "keypoints = []\n",
    "descriptors = []\n",
    "for img in images_files:\n",
    "    img_g = cv.imread(images_folder + \"/\" + img, cv.IMREAD_GRAYSCALE)\n",
    "    if img_g is None:\n",
    "        print(\"No se pudo abrir la imagen \" + img)\n",
    "        exit(0)\n",
    "\n",
    "    # Inicializar el descriptor AKAZE y detectar keypoints y extraer los descriptores de la imagen.\n",
    "    # kpts -> keypoints\n",
    "    # descs -> descriptores\n",
    "    kpts, descs = akaze.detectAndCompute(img_g, None)\n",
    "    print(\"keypoints: {}, descriptors: {}\".format(len(kpts), descs.shape))\n",
    "\n",
    "    # Crear la clase imagen con sus keypoints y descriptors\n",
    "    # Se incluye en el diccionario de imágenes\n",
    "    images[img] = Image(img, img_g, kpts, descs)\n",
    "\n",
    "    # Mostrar resultados de la extracción\n",
    "    # showKeypoints(img_g, kpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El match final tiene menos de la mitad de matches que el original. Ignorando imagen.\n",
      "El match final tiene menos de la mitad de matches que el original. Ignorando imagen.\n",
      "El match final tiene menos de la mitad de matches que el original. Ignorando imagen.\n"
     ]
    }
   ],
   "source": [
    "# Fase de Matching\n",
    "\n",
    "# Crear matches para todas las imágenes\n",
    "# Utilizamos distancia de Hamming porque AKAZE es un descriptor binario\n",
    "matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_BRUTEFORCE_HAMMING)\n",
    "\n",
    "for i, img_i in enumerate(list(images.values())):\n",
    "    for j, img_j in enumerate(list(images.values())[i + 1 : :]):\n",
    "        # Encontrar los match\n",
    "        matches = matchWithRatioTest(\n",
    "            matcher, img_i.descriptors, img_j.descriptors\n",
    "        )\n",
    "\n",
    "        # Test de reciprocidad\n",
    "        matchRcp = matchWithRatioTest(\n",
    "            matcher, img_j.descriptors, img_i.descriptors\n",
    "        )\n",
    "\n",
    "        merged_matches = []\n",
    "\n",
    "        for mR_m in matchRcp:\n",
    "            found = False\n",
    "            for m_m in matches:\n",
    "                # Solo se acepta el match si es recíproco\n",
    "                if (\n",
    "                    mR_m.queryIdx == m_m.trainIdx\n",
    "                    and mR_m.trainIdx == m_m.queryIdx\n",
    "                ):\n",
    "                    merged_matches.append(m_m)\n",
    "                    found = True\n",
    "                    break\n",
    "                if found:\n",
    "                    continue\n",
    "\n",
    "        # Restricción Epipolar. Calculamos la matriz fundamental utilizando RANSAC\n",
    "        img_i_pts = []\n",
    "        img_j_pts = []\n",
    "\n",
    "        for m in merged_matches:\n",
    "            img_i_pts.append(img_i.keypoints[m.queryIdx].pt)\n",
    "            img_j_pts.append(img_j.keypoints[m.trainIdx].pt)\n",
    "\n",
    "        F, inliersMask = cv.findFundamentalMat(\n",
    "            np.array([img_i_pts]), np.array([img_j_pts]), cv.FM_RANSAC\n",
    "        )\n",
    "\n",
    "        final = []\n",
    "\n",
    "        for mask, match in zip(inliersMask, merged_matches):\n",
    "            if mask:\n",
    "                final.append(match)\n",
    "\n",
    "        if float(len(final)) / float(len(matches)) < PAIR_MATCH_SURVIVAL_RATE:\n",
    "            print(\n",
    "                \"El match final tiene menos de la mitad de matches que el original. Ignorando imagen.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        images[img_i.img_name].matches[\n",
    "            (img_i.img_name, img_j.img_name)\n",
    "        ] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de componentes: 642\n",
      "Componentes buenos: 642\n"
     ]
    }
   ],
   "source": [
    "# Crear grafo de características para rastrearlas\n",
    "\n",
    "vertexByImageFeature = {}\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Agregamos nodos con las imágenes\n",
    "for img_tpl in images.values():\n",
    "    for i in range(len(img_tpl.keypoints)):\n",
    "        node = ImageFeature(img_tpl.img_name, i)\n",
    "        G.add_node(node)\n",
    "        vertexByImageFeature[(img_tpl.img_name, i)] = node\n",
    "\n",
    "# Agregamos las aristas (matches)\n",
    "for img_tpl in images.values():\n",
    "    for matches in img_tpl.matches:\n",
    "        for m in img_tpl.matches[matches]:\n",
    "            v1 = vertexByImageFeature[(matches[0], m.queryIdx)]\n",
    "            v2 = vertexByImageFeature[(matches[1], m.trainIdx)]\n",
    "            G.add_edge(v1, v2)\n",
    "\n",
    "# Eliminar nodos sin aristas\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "# Obtener los componentes conectados (retorna una lista de sets con nodos (componentes))\n",
    "components = nx.connected_components(G)\n",
    "goodComponents = list()\n",
    "# Filtramos los componentes (más de 1 característica por imagen)\n",
    "for c in components:\n",
    "    isGoodComponent = True\n",
    "    imagesInComponent = set()\n",
    "    for img in c:\n",
    "        img_name = img.img_name\n",
    "        if img_name in imagesInComponent:\n",
    "            # La imagen ya se encuentra en el componente\n",
    "            isGoodComponent = False\n",
    "            break\n",
    "        else:\n",
    "            imagesInComponent.add(img_name)\n",
    "    if isGoodComponent:\n",
    "        goodComponents.append(c)\n",
    "\n",
    "print(f\"Total de componentes: {nx.number_connected_components(G)}\")\n",
    "print(f\"Componentes buenos: {len(goodComponents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de Tracks para alimentar sfm\n",
    "tracks = {}\n",
    "\n",
    "# Creamos un array de tracks inicializados en -1\n",
    "for img in images_files:\n",
    "    tracks[img] = np.full((2, len(goodComponents)), -1.0, np.float64)\n",
    "\n",
    "\n",
    "for (i,c) in enumerate(goodComponents):\n",
    "    for img in c:\n",
    "        img_name = img.img_name\n",
    "        kpt_idx = img.kpt_idx\n",
    "        p = images[img_name].keypoints[kpt_idx].pt\n",
    "        tracks[img_name][0][i] = p[0]\n",
    "        tracks[img_name][1][i] = p[1]\n",
    "\n",
    "tracks_list = np.fromiter(tracks.values(), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.49071631e+03,  3.12157007e+03,  3.18299341e+03, ...,\n",
       "        -1.00000000e+00, -1.00000000e+00, -1.00000000e+00],\n",
       "       [ 1.39102271e+03,  1.42428687e+03,  1.59039038e+03, ...,\n",
       "        -1.00000000e+00, -1.00000000e+00, -1.00000000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function reconstruct:\n",
      "\n",
      "reconstruct(...)\n",
      "    reconstruct(points2d, K[, Rs[, Ts[, points3d[, is_projective]]]]) -> Rs, Ts, K, points3d\n",
      "    .   @brief Reconstruct 3d points from 2d correspondences while performing autocalibration.\n",
      "    .     @param points2d Input vector of vectors of 2d points (the inner vector is per image).\n",
      "    .     @param Rs Output vector of 3x3 rotations of the camera.\n",
      "    .     @param Ts Output vector of 3x1 translations of the camera.\n",
      "    .     @param points3d Output array with estimated 3d points.\n",
      "    .     @param K Input/Output camera matrix \\f$K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$. Input parameters used as initial guess.\n",
      "    .     @param is_projective if true, the cameras are supposed to be projective.\n",
      "    .\n",
      "    .     Internally calls libmv simple pipeline routine with some default parameters by instatiating SFMLibmvEuclideanReconstruction class.\n",
      "    .\n",
      "    .     @note\n",
      "    .       - Tracks must be as precise as possible. It does not handle outliers and is very sensible to them.\n",
      "    .       - To see a working example for camera motion reconstruction, check the following tutorial: @ref tutorial_sfm_trajectory_estimation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv.sfm.reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgS = images[list(images.keys())[0]].image.shape\n",
    "\n",
    "f = np.float64(np.max(imgS))\n",
    "\n",
    "\n",
    "# Camera Matrix\n",
    "# TODO: Incluir la calibración de la cámara en este NB\n",
    "K = np.array([[f, 0.0, imgS[1]/2.0], [0.0, f, imgS[0] / 2.0], [0.0, 0.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs, Ts, K, points3d = cv.sfm.reconstruct(points2d=tracks_list, K=K, is_projective=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstrucción:\n",
      "Puntos 3D estimados: 642\n",
      "Rotaciones estimadas de camara: 4\n",
      "Traslaciones estimadas de camara: 4\n",
      "Parametros intrinsecos refinados: \n",
      "[[3.93311136e+03 0.00000000e+00 2.68810129e+03]\n",
      " [0.00000000e+00 3.93311136e+03 1.57017989e+03]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reconstrucción:\")\n",
    "print(\"Puntos 3D estimados: \"+str(len(points3d)))\n",
    "print(\"Rotaciones estimadas de camara: \" + str(len(Rs)))\n",
    "print(\"Traslaciones estimadas de camara: \" + str(len(Ts)))\n",
    "print(\"Parametros intrinsecos refinados: \")\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 3, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir a un array de vectores de 3x1 (Puntos 3D)\n",
    "pointCloud = np.array(points3d)\n",
    "pointCloud.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener los colores de los puntos\n",
    "defaultColor = np.array([0,255,0])\n",
    "pointCloudColor = np.tile(defaultColor, (pointCloud.shape[0], 1))\n",
    "pointCloudColor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buscar el color del punto en todas las imágenes\n",
    "for i,point in enumerate(pointCloud):\n",
    "    for img, R, T in zip(images.values(), Rs, Ts):\n",
    "        point3d = np.reshape(point, (3,))\n",
    "        point2d = cv.projectPoints(point3d, R, T, K, None)\n",
    "        # Si el punto se encuentra fuera de los límites\n",
    "        x = int(point2d[0][0][0][0])\n",
    "        y = int(point2d[0][0][0][1])\n",
    "       \n",
    "        if (\n",
    "            x < 0\n",
    "            or x >= img.image.shape[0]\n",
    "            or y < 0\n",
    "            or y >= img.image.shape[1]\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        pointCloudColor[i] = np.array(img.image[x, y])\n",
    "        break\n",
    "\n",
    "\n",
    "pointCloudColor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar en ventana 3D\n",
    "window = cv.viz.Viz3d(\"Frame de Coordenadas\")\n",
    "window.setWindowSize((500,500))\n",
    "window.setWindowPosition((150,150))\n",
    "window.setBackgroundColor(cv.viz.Color.white())\n",
    "\n",
    "# Recuperar camaras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93502787,  0.03895333, -0.35242803,  1.21109839],\n",
       "       [-0.03841536,  0.99922551,  0.00852295, -0.04044635],\n",
       "       [ 0.35248707,  0.00556945,  0.93580011,  0.22705069],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Affine = np.concatenate((Rs[0], Ts[0]), axis = 1)\n",
    "Affine = np.concatenate((Affine, np.array([[0,0,0,1]])))\n",
    "Affine\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
